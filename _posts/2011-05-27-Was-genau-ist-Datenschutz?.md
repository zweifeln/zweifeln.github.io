---
title: "Was genau ist Datenschutz?"
layout: post
categories: 
---
Der Datenschutz hat sich lange vor der Privatsphären-Diskussion á la Facebook, Google und co gebildet. Er hat sich auch nicht aus Angst um die "Privatsphäre" gebildet, sondern aus Angst vor der technischen Fremdbestimmung. In den 80ern waren KI-Forscher (Künstliche Intelligenz) deutlich euphorischer als Heute und verkündeten, dass in 10 Jahren Computer das selbe könnte, wie Menschen. Es gab den <a href="http://de.wikipedia.org/wiki/Turing-Test">Turing-Test</a> und dieser sollte innerhalb von 10 Jahren bestanden werden. Heute, 30 Jahre später, hat es noch immer keine Maschine geschafft diesen Test zu bestehen.

Mit der Angst vor diesen denkenden Maschinen kam der Datenschutz auf. Damit hat Julia Schramm recht, wenn sie behauptet, <a href="http://www.spiegel.de/netzwelt/netzpolitik/0,1518,749831,00.html">Datenschutz ist sowas von Eighties</a>. Damit aber war nicht der Schutz der Privatsphäre vor anderen Menschen, sondern vor der Maschine gemeint...

Heute nun, sind wir deutlich weniger euphorisch, was die denkende Maschine angeht. Es gibt zwar einige interessante Ansätze, wie die <a href="http://de.wikipedia.org/wiki/K%C3%BCnstliches_neuronales_Netz">Künstlichen Neuronalen Netze</a>, allerdings gibt es auch gute Gegenargumente, wie das <a href="http://de.wikipedia.org/wiki/Chinesisches_Zimmer">Chinesische Zimmer</a> von John Searle. Das Chinesische Zimmer zielt darauf ab, wie Semantik in der syntaktischen Funktionsweise eines Computers möglich ist. Wie der Computer es schaffen kann ein Bewusstsein zu bekommen - was sehr schwer werden dürfte. Searle behauptet, es funktioniert nicht. Ich bin da auch seeeehr skeptisch (denn so viel ich vom Gehirn verstanden habe, ist es doch deutlich komplexer als jede technische Struktur).

Festhalten lässt sich, dass wir zwar zunehmend mehr mit Technik und Computer arbeiten (und in 10 Jahren noch viel mehr als heute), doch wird der Computer so schnell noch nicht für uns denken. Er wird noch immer nur machen, was wir von ihm verlangen - allerdings gibt es eine neue Macht. Die Macht des Algorithmus. Wir nutzen mehr und mehr Algorithmen, ohne zu verstehen, wie genau diese funktionieren. Wir vertrauen den Algorithmen und hoffe, dass sie das machen, was wir von ihnen wollen. Sicher sein können wir uns dabei allerdings nicht.

Das ist der Punkt, wo der Datenschutz heute greift. Wer nicht weiß, was mit ihm (und seinen Daten) passiert, geht im Allgemeinen eher vorsichtig vor. Das passt im Allgemeinen zu der vor-digital-natives-Generation. Wir alle kennen Menschen, die ohne Internet aufgwachsen sind und diese haben eine unglaubliche Angst davor, überhaupt irgendwo etwas ins Internet zu schreiben. Da gibt es kein Facebook, kein Twitter und erst recht kein Blog.

Der Großteil der digital-natives hat aber auch keine Ahnung, wie die Algorithmen selbst aussehen, bilden sich aber ein, das Internet "verstanden" zu haben. Als einer dieser, sehe auch ich mich. Wir meinen zu wissen, was auf Facebook passiert und schreiben da freiwillig rein - allerdings mit einer Intention. Wir "wollen" Nachrichten über uns verbreiten und uns mitteilen. Um mit Freunden noch in Kontakt zu bleiben, bleibt kaum eine andere Möglichkeit, als social networks zu nutzen. Die Zeit dreht sich schneller - wir leben in einer so kras beschleunigten Welt, dass wir uns nur noch digital verabreden können. Früher ist man einfach zu seinen Freunden gegangen und konnte was zusammen machen. Heute treffen wir unsere Freunde doch gar nicht an, wenn wir vor deren Tür stehen. Dafür sind wir zu viel unterwegs. Daher, so mutmaße ich, kommt es, dass wir uns immer mehr digital unterhalten und social networks nutzen.

Der heutige Datenschutz soll genau dieses ja auch nicht unterbinden, sondern sicher stellen, dass wir nur das, was wir mit Freunden austauschen wollen, auch austauschen. Der Datenschutz soll zum einen das Profil unterbinden, dass die Algorithmen von uns erstellen (und damit Macht über uns ausüben, weil sie (so lange wir nicht genau wissen, was die Algorithmen machen) bestimmen, was wir zu sehen bekommen und was nicht. So lange es nicht unsere eigenen Filter sind, sind es fremde Filter, die vor allem eins wollen: Geld.

Dieser Datenschutz wehrt sich gegen die Vorratsdatenspeicherung, den Facebook-like-button und Google. Hier sehe ich auch bei Datenschutzkritikern durchaus parallelen.

Das, was es aber auch gibt, ist der Datenschutzfundametalismus. Es solle kein Häuserfront im Internet auftauchen und man traut es sich auch nicht, überhaupt irgendwo was hin zu schreiben. Die Angst vor den Party-Kotze-Fotos. Hier allerdings kann man es gut runterbrechen auf das "Jeder stellt das von sich ins Netz, was er da haben will", die informationelle Selbstbestimmung.
Weshalb aber auch dieser Ansatz <a href="http://deliberationfront.wordpress.com/2011/05/18/technik-frisst-privatsphare-zweithirn-fur-spackos/">mit dem Technikdeterminismus negiert wird</a>, verstehe ich nicht. Wie oben schon (anhand der KI) aufgezeigt, gibt es viele Vorstellungen von der Technik, die nicht eingehalten werden. Technik hat Grenzen. Technik schafft es nicht \*alle\* Informationen allen verfügbar zu machen. Wird sie auch nie. Die Informationen müssen eh noch immer von Hand der Technik zur Verfügung gestellt werden - dann kann man sich auch im Miteinander einigen, dass jeder nur das einstellt, was ihn betrifft. Der Rest kommt einfach nicht rein (oder wird gepixelt). Beim (materiellen) Eigentum funktioniert es ja auch. Nur weil es möglich ist ein Fahrrad zu klauen tue ich es nicht. Man nennt sowas eine "Ethik" und diese Ethik funktioniert so weit sehr gut im Netz. Sie könnte besser funktionieren - und sollte es auch - aber sie funktioniert, wie es auch funktioniert, dass angeschlossene Fahrräder nicht geklaut werden - auch wenn man das Schloss (technisch) knacken kann. Es passiert doch nur sehr selten.
		

__Kommentare__
			
_von: \*manU_
			
Danke für Deine Erwähnung... allerdings missverstehst Du mich leider etwas, wenn Du annimmst, ich argumentiere _gegen_ Datenschutz "mit dem Technikdeterminismus". Lediglich meine Einleitung bezieht sich auf die Tatsache, dass der Mensch bisher so ziemlich Alles denkbare und Vorstellbare realisiert hat, auch wenn es der Gesellschaft vor der Implementation vielleicht noch abwegig erschienen sein mag. 
Ich glaube also tatsächlich, dass "Verstecken" immer schwieriger wird. 

Aber das ist nicht der eigentliche Punkt meiner Ausführungen:
Im Grunde ziele ich darauf ab, unsere individuelle ethische Sensibilität im Bezug auf den Umgang mit Daten zu schärfen. Ganz einfach gesagt, sollten wir darauf Hinwirken, dass Datenmissbrauch an sich zur Peinlichkeit wird. Wir müssen verstehen und verständlich machen, dass derjenige, der Daten für fragwürdige Zwecke entwendet und missbraucht, in einer zunehmend transparenteren und offeneren Gesellschaft  immer auch selbst viel eher dazu gezwungen ist, seine Ideen und Handlungen vor einer wachen Öffentlichkeit vertreten zu können. 

Allgemein zunehmende Transparenz ist ein zweischneidiges Schwert! 
Das schneidet nicht nur unbedarfte und unbescholtene Bürger, sondern auch den, der sich daneben benimmt! Das sollten wir im Rahmen der Diskussion um dieses Thema nicht ausklammern! Die Kontroverse Datenschutz vs. Post-Privacy wird uns doch eigentlich eher aufgesetzt... Die eigentliche Kontroverse bezieht sich auf Datenschutz/ Datenmissbrauch!
<strong>In der zunehmende Transparenz ergeben sich eben auch inhärente Faktoren der Regulation, da Fehlverhalten ja auch viel eher bloßgestellt werden kann (und sollte!)... </strong> 
Wir haben ja bereits einige schöne stigmatisierende Begriffe, die Personen bezeichnen, die sich im Umgang mit Daten und Informationen daneben benehmen: <i>Niemand möchte als "Spitzel", "Lauscher", "Spanner", "Lästermaul", "Schlange" oder "Klatschtante" verschrien werden... Diese Stigmata dürfen wir angesichts Datenmissbrauch gut und gerne hochhalten!</i> ;)

Was mich hoffnungsvoll stimmt ist ein Prozess der allgemeinen "Läuterung" unserer Motive und Absichten angesichts der wachsenden "Gefahr", dass wir uns blamieren, wenn wir uns anders verhalten, als wir es von unserem Gegenüber erwarten oder vor der Öffentlichkeit vertreten können. 

Wie ich in meinem <a href="http://deliberationfront.wordpress.com/2011/05/18/technik-frisst-privatsphare-zweithirn-fur-spackos/" rel="nofollow">Blogartikel</a> geschrieben habe:
&gt;&gt; Uns sollte bewusst werden, dass auch diejenigen Subjekte, die eine (wie auch immer geartete) Form der „Überwachung“, Kontrolle (und Manipulation) auf andere Gruppen und Individuen ausüben wollen, ihre eigenen Normen, Ansichten und Auffassungen (ihre Realität!) an eben den Normen, Ansichten und Auffassungen relativieren müssen/ werden, die in der Öffentlichkeit „gehandelt“ und ‘konsolidiert’ werden. \[Wir alle sind am Prozess dieser Werte-bildung (der Evaluation der Werte) beteiligt!\] … wenn wir uns einbringen wollen (können?).&lt;&lt;

_____________________________________________________________________
Du erwähnst in Deinem Beitrag auch das &quot;Chinesische Zimmer&quot;. und John Searle.. :)

Künstliche Intelligenz ist ein komplexes (oft kompliziertes) Thema, das vielleicht erst in zweiter Linie damit zu tun hat, aber durchaus interessant ist. 

Ich lese gerade das Buch: &quot;<a href="http://books.google.com/books/about/The_Emotion_Machine.html?id=OqbMnWDKIJ4C" rel="nofollow">The Emotion Machine</a>".
<strong>Marvin Minsky</strong> meint, dass \*Komplexität\* an sich aber kein Hinderungsgrund für eine durchaus erfolgreiche Implementation künstlicher Intelligenz sein muss. 

http://youtu.be/SNWVvZi3HX8
&amp; 
http://youtu.be/S43WaQywDbw

